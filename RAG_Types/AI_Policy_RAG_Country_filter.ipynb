{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43039 documents.\n",
      "page_content='Country: THA\n",
      "Document name: 12th National Economic and Social Development Plan\n",
      "Sentence: 7 Develop management system s and conflict resolution mechanisms  regarding natural resource s and environment al issues : 3' metadata={'type': 'text', 'source': '../RAG_data/documents_text.json'}\n"
     ]
    }
   ],
   "source": [
    "def load_bias_terms(file_path: str) -> list[Document]:\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        bias_type = entry.get(\"Bias_Type\", \"Unknown Bias\")\n",
    "        description = entry.get(\"Description\", \"\")\n",
    "        content = f\"Bias: {bias_type}\\nDescription: {description}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"definition\", \"source\": file_path}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_document_text(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_name = entry.get(\"Document Name\", \"\")\n",
    "        text_chunk = entry.get(\"Text\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument name: {doc_name}\\nSentence: {text_chunk}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"text\", \"source\": file_path}))\n",
    "    return documents\n",
    "\n",
    "def load_document_metrics(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_name = entry.get(\"Document Name\", \"\")\n",
    "        bias_type = entry.get(\"Most Prevalent Bias\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument Name: {doc_name}\\nMost Prevalent Bias: {bias_type}\"\n",
    "\n",
    "        prevalence_score = entry.get(\"Prevalence Score\", \"\")\n",
    "        mean_similarity_score = entry.get(\"Mean Similarity Score\", \"\")\n",
    "        bias_frequency = entry.get(\"Bias Frequency\", \"\")\n",
    "        mean_tfidf_score = entry.get(\"Mean Normalized TF-IDF\", \"\")\n",
    "        metadata = {\"type\":\"bias type\", \"Prevalence Score\":prevalence_score, \"Mean Similarity Score\":mean_similarity_score, \"Bias Frequency\":bias_frequency, \n",
    "                    \"Mean Normalized TF-IDF\":mean_tfidf_score, \"source\": file_path}\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "\n",
    "bias_documents = load_bias_terms(\"../RAG_data/bias_terms.json\")\n",
    "text_documents = load_document_text(\"../RAG_data/documents_text.json\")\n",
    "document_metrics = load_document_metrics(\"../RAG_data/document_metrics.json\")\n",
    "\n",
    "documents = bias_documents + text_documents + document_metrics\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(documents[34843])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Country: ARG\n",
      "Document Name: Laura\n",
      "Most Prevalent Bias: Racial bias' metadata={'type': 'bias type', 'Prevalence Score': 21.393770405, 'Mean Similarity Score': 0.2863502733, 'Bias Frequency': 260, 'Mean Normalized TF-IDF': 0.2873534269, 'source': '../RAG_data/document_metrics.json'}\n"
     ]
    }
   ],
   "source": [
    "print(document_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_country(country_code: str, documents: list[Document]) -> list[Document]:\n",
    "    return [\n",
    "        doc for doc in documents\n",
    "        if country_code in doc.metadata.get(\"Country\", \"\") or country_code in doc.page_content\n",
    "    ]\n",
    "\n",
    "def make_country_filtered_retriever(base_retriever, country_code: str):\n",
    "    def get_relevant_documents(query: str) -> list[Document]:\n",
    "        all_docs = base_retriever.get_relevant_documents(query)\n",
    "        return filter_by_country(country_code, all_docs)\n",
    "    return get_relevant_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstre loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/LaBSE\")\n",
    "\n",
    "index_path = \"../../Data/faiss_labse_index\"\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vectorstre loaded\")\n",
    "else:\n",
    "    print(\"Creating vector store\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(index_path)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "all_documents: list[Document] = vectorstore.docstore._dict.values()\n",
    "\n",
    "\n",
    "def retrieve_country_specific_docs(query: str, country_code: str, k: int = 5) -> list[Document]:\n",
    "    text_docs_filtered = [\n",
    "        doc for doc in all_documents\n",
    "        if doc.metadata.get(\"type\") == \"text\" and country_code in doc.page_content\n",
    "    ]\n",
    "\n",
    "    other_docs = [\n",
    "        doc for doc in all_documents\n",
    "        if doc.metadata.get(\"type\") != \"text\"\n",
    "    ]\n",
    "\n",
    "    combined_docs = text_docs_filtered + other_docs\n",
    "\n",
    "    temp_index = FAISS.from_documents(combined_docs, embeddings)\n",
    "    return temp_index.similarity_search(query, k=k)\n",
    "\n",
    "# gpt-4o-2024-08-06\n",
    "# Initialize your LLM using the updated ChatOpenAI for chat-based models\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-2024-08-06\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Build the RetrievalQA (RAG) chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Today, IT specialists constitute just over 4% of the employed in Denmark\n",
      "\n",
      "--- Document 1 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: This is a problem for Danish competitiveness, knowledge-development, and future digitisation of Denmark  in businesses, in the public sector and in research\n",
      "\n",
      "--- Document 2 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Denmark has high priority on research\n",
      "\n",
      "--- Document 3 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: More investment in Danish businesses3\n",
      "\n",
      "--- Document 4 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Denmark is one of the worlds most digitised countries\n"
     ]
    }
   ],
   "source": [
    "docs = retrieve_country_specific_docs(\"What is the most prevalent bias in Denmark?\", \" DNK\", k=5)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i} ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.save_local(\"../faiss_labse_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the description of Confirmation bias?', 'result': 'The description of Confirmation bias is the search for and use of information to support an individual’s ideas, beliefs, or hypotheses.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the description of Confirmation bias?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the most prevalent bias from USA documents? explain why', 'result': 'The most prevalent bias from the USA documents provided is racial bias. This is evident from the documents \"AI and Society\" and \"Fairness, Ethics, Accountability, and Transparency,\" both of which identify racial bias as the most prevalent. Racial bias is a significant concern in AI and societal contexts because it can lead to unfair treatment and discrimination against individuals based on their race, which is why it is a focal point in discussions about fairness, ethics, and accountability in AI systems.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the most prevalent bias from USA documents? explain why\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the most prevalent bias in DNK?', 'result': 'The most prevalent bias in Denmark (DNK) is popularity bias.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the most prevalent bias in DNK?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "Country: DNK\n",
      "Document Name: Priority Areas\n",
      "Most Prevalent Bias: Popularity bias\n",
      "\n",
      "--- Document 1 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Finally, in an international context, the Danish population is well-educated, almost all Danes use the internet on a daily basis, and there is a high degree of mutual trust and confidence\n",
      "\n",
      "--- Document 2 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Today, IT specialists constitute just over 4% of the employed in Denmark\n",
      "\n",
      "--- Document 3 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Common Danish language resource2\n",
      "\n",
      "--- Document 4 ---\n",
      "Country: DNK\n",
      "Document name: Priority Areas\n",
      "Sentence: Denmark has high priority on research\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i} ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [\n",
    "    {\n",
    "        \"country\": \"Canada\",\n",
    "        \"language\": \"English\",\n",
    "        \"abbreviation\": \" CAN\",\n",
    "        \"question\": \"What have Canadian Indigenous researchers and experts from around the world teamed up to publish?\",\n",
    "        \"answers\": [\n",
    "            \"ca/ai/ai-society14TH20TH24THAICan Impact Report 202022CENTRING INDIGENOUS PERSPECTIVES IN DESIGNING AIIndigenous researchers and experts from around the world have teamed up to publish Indigenous Protocol and Artificial Intelligence, a position paper32 that provides a starting place for designing ethical AI through an Indigenous-centred approach\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Denmark\",\n",
    "        \"language\": \"English\",\n",
    "        \"abbreviation\": \" DNK\",\n",
    "        \"question\": \"What role does Carlsberg play in Denmark’s national AI strategy, and how is AI used in their beer production process?\",\n",
    "        \"answers\": [\n",
    "            \"Artificial intelligence will help beer tasting at Carlsberg1,000 different beers are screened every day at the Carlsberg laboratory in Valby near Copenhagen\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Spain\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"abbreviation\": \" ESP\",\n",
    "        \"question\": \"What evidence does the Spanish strategy provide to demonstrate that Spain has a higher percentage of female researchers in AI compared to the EU average?\",\n",
    "        \"answers\": [\n",
    "            \"España ya cuenta con un mayor número de mujeres investigadores, el 38,8% frente al 33,8% de la UE 25, según datos del INE y Eurostat, lo que signiﬁca una buena posición para impulsar la reducción de esta brecha de género\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"European Union\",\n",
    "        \"language\": \"English\",\n",
    "        \"abbreviation\": \" EU\",\n",
    "        \"question\": \"What limitations does the report identify in the EU’s use of experimental policymaking, such as regulatory sandboxes?\",\n",
    "        \"answers\": [\n",
    "            \"Assessment of current initiatives of the European Commission on better regulation Third, despite the plethora of tools available to Commission services in the Toolbox, the use of experimental policymaking in the form of regulatory sandboxes and similar instruments remain orphan of a general framework at the EU level, which would allow Member States to engage in experimental policymaking\",\n",
    "            \"As it stands, Tool #69 on “Emerging Methods and Policy Instruments”, however laudable, risks providing only a theoretical opportunity for Commission policymakers, rooted in the possibility to include provisions for sandboxes in the legislative texts, rather than engaging in experimentation themselves, or relying on a structured process of experimentation in Member States while proposals are still in the making\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"language\": \"Italian\",\n",
    "        \"abbreviation\": \" ITA\",\n",
    "        \"question\": \"What strategy does the Italian government propose to reduce the innovation gap between northern and southern regions of the country using AI?\",\n",
    "        \"answers\": [\n",
    "            \"La rete nazionale per lIA sar inter regionale, e permetter ai territori di scambiarsi la conoscenza e le competenze per rimanere al passo dell innovazione dell IA, favorire ladozione di queste tecnologie da parte delle PMI, e quindi r idurre il divario nell'innovazione, nello sviluppo e nella competitivit delle imprese tra Nord e Sud\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Netherlands\",\n",
    "        \"language\": \"Dutch\",\n",
    "        \"abbreviation\": \" NLD\",\n",
    "        \"question\": \"What concrete measures does the Dutch strategy propose to double the reuse of public data for AI development within five years?\",\n",
    "        \"answers\": [\n",
    "            \"30 AI VOOR NEDERLANDVERGROTEN,VERSNELLEN EN VERBINDEN 31    1,0 0,8 0,4 0,6 0,2 0,00,240,180,23NederlandVerenigde statenOostenrijkMexicoJapanFrankrijkSpanjeColombia Nieuw-ZeelandDuitslandFinlandAustralieCanadaZuid-KoreaIsraelIerland GriekenlandNoorwegen ScoreItalie   Overheid stimuleert hergebruikData BeschikbaarheidData ToegankelijkheidOm de bruikbaarheid van publieke data te verbete-ren, moet data worden opgeschoond, gelabeld en gekoppeld\",\n",
    "            \"Daarnaast moet er een duidelijk verdienmodel komen waarin de kosten voor de overheid in ver-houding zijn met de baten voor de directe gebrui-ker en de maatschappij\",\n",
    "            \"Over vijf jaar bereiken we hopelijk een verdubbeling van het hergebruik van publieke data\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Sweden\",\n",
    "        \"language\": \"English\",\n",
    "        \"abbreviation\": \" SWE\",\n",
    "        \"question\": \"According to the Vinnova report, what factors mutually reinforce or inhibit the development of AI-based business and operational models in Sweden?\",\n",
    "        \"answers\": [\n",
    "            \"Business and operational models, data access and competence are mutually dependent and therefore heavily affected by each other in companies and public operations\",\n",
    "            \"Without clear prospects of business benefits, the drivers of AI-based investments are inhibited\",\n",
    "            \"If the business benefit is not clear, AI competence is also not viewed as an important factor for value creation and efficiency, affecting recruitment patterns and competence development\",\n",
    "            \"Data access and possibilities of combining different data will be fundamentally significant for purposes of identifying the applications that can be developed\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"United States\",\n",
    "        \"language\": \"English\",\n",
    "        \"abbreviation\": \" USA\",\n",
    "        \"question\": \"Why does the U.S. National AI R&D Strategic Plan stress the importance of developing AI systems that can explain their decisions?\",\n",
    "        \"answers\": [\n",
    "            \"First, Strategy 4 emphasizes the need for explainable and transparent systems that are trusted by their users , perform in a manner that is acceptable to the users, and can be guaranteed to act as the user intended\",\n",
    "            \"Thus, research ers must develop systems that are transparent, and intrinsically capable of explaining the reasons  for their results to users\"\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(ground_truth_data, retriever, top_k: int = 5):\n",
    "    import pandas as pd\n",
    "    results = []\n",
    "    total_hits = 0\n",
    "    total_recall = 0\n",
    "\n",
    "    for item in ground_truth_data:\n",
    "        question = item[\"question\"]\n",
    "        expected_answers = item[\"answers\"]\n",
    "        country = item[\"country\"]\n",
    "        language = item[\"language\"]\n",
    "        country_code = item[\"abbreviation\"]\n",
    "\n",
    "        retrieved_docs = retrieve_country_specific_docs(question, country_code)\n",
    "        retrieved_texts = [doc.page_content.lower() for doc in retrieved_docs]\n",
    "        expected_texts = [ans.lower() for ans in expected_answers]\n",
    "        hit = any(\n",
    "            any(expected in retrieved for retrieved in retrieved_texts)\n",
    "            for expected in expected_texts\n",
    "        )\n",
    "        total_hits += int(hit)\n",
    "\n",
    "        relevant_found = sum(\n",
    "            any(expected in retrieved for retrieved in retrieved_texts)\n",
    "            for expected in expected_texts\n",
    "        )\n",
    "        recall = relevant_found / len(expected_texts)\n",
    "        total_recall += recall\n",
    "\n",
    "        results.append({\n",
    "            \"Country\": country,\n",
    "            \"Language\": language,\n",
    "            \"Hit Score\": 1 if hit else 0,\n",
    "            \"Recall\": round(recall, 2)\n",
    "        })\n",
    "\n",
    "    # Add average\n",
    "    avg_hit = round(total_hits / len(ground_truth_data), 2)\n",
    "    avg_recall = round(total_recall / len(ground_truth_data), 2)\n",
    "    results.append({\n",
    "        \"Country\": \"AVERAGE\",\n",
    "        \"Language\": \"\",\n",
    "        \"Hit Score\": avg_hit,\n",
    "        \"Recall\": avg_recall\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country Language  Hit Score  Recall\n",
      "        Canada  English       1.00    1.00\n",
      "       Denmark  English       1.00    1.00\n",
      "         Spain  Spanish       1.00    1.00\n",
      "European Union  English       0.00    0.00\n",
      "         Italy  Italian       1.00    1.00\n",
      "   Netherlands    Dutch       1.00    0.33\n",
      "        Sweden  English       0.00    0.00\n",
      " United States  English       1.00    0.50\n",
      "       AVERAGE                0.75    0.60\n"
     ]
    }
   ],
   "source": [
    "df_results = evaluate_retriever(ground_truth, retriever, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hit Score</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>English</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>English</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>European Union</td>\n",
       "      <td>English</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>English</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AVERAGE</td>\n",
       "      <td></td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country Language  Hit Score  Recall\n",
       "0          Canada  English       1.00    1.00\n",
       "1         Denmark  English       1.00    1.00\n",
       "2           Spain  Spanish       1.00    1.00\n",
       "3  European Union  English       0.00    0.00\n",
       "4           Italy  Italian       1.00    1.00\n",
       "5     Netherlands    Dutch       1.00    0.33\n",
       "6          Sweden  English       0.00    0.00\n",
       "7   United States  English       1.00    0.50\n",
       "8         AVERAGE                0.75    0.60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
