{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 43039 documents.\n",
      "page_content='Country: THA\n",
      "Document name: 12th National Economic and Social Development Plan\n",
      "Sentence: 7 Develop management system s and conflict resolution mechanisms  regarding natural resource s and environment al issues : 3' metadata={'type': 'text', 'source': '../RAG_data/documents_text.json'}\n"
     ]
    }
   ],
   "source": [
    "def load_bias_terms(file_path: str) -> list[Document]:\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        bias_type = entry.get(\"Bias_Type\", \"Unknown Bias\")\n",
    "        description = entry.get(\"Description\", \"\")\n",
    "        content = f\"Bias: {bias_type}\\nDescription: {description}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"definition\", \"source\": file_path}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_document_text(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_name = entry.get(\"Document Name\", \"\")\n",
    "        text_chunk = entry.get(\"Text\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument name: {doc_name}\\nSentence: {text_chunk}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"text\", \"source\": file_path}))\n",
    "    return documents\n",
    "\n",
    "def load_document_metrics(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_name = entry.get(\"Document Name\", \"\")\n",
    "        bias_type = entry.get(\"Most Prevalent Bias\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument Name: {doc_name}\\nMost Prevalent Bias: {bias_type}\"\n",
    "\n",
    "        prevalence_score = entry.get(\"Prevalence Score\", \"\")\n",
    "        mean_similarity_score = entry.get(\"Mean Similarity Score\", \"\")\n",
    "        bias_frequency = entry.get(\"Bias Frequency\", \"\")\n",
    "        mean_tfidf_score = entry.get(\"Mean Normalized TF-IDF\", \"\")\n",
    "        metadata = {\"type\":\"bias type\", \"Prevalence Score\":prevalence_score, \"Mean Similarity Score\":mean_similarity_score, \"Bias Frequency\":bias_frequency, \n",
    "                    \"Mean Normalized TF-IDF\":mean_tfidf_score, \"source\": file_path}\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "\n",
    "bias_documents = load_bias_terms(\"../RAG_data/bias_terms.json\")\n",
    "text_documents = load_document_text(\"../RAG_data/documents_text.json\")\n",
    "document_metrics = load_document_metrics(\"../RAG_data/document_metrics.json\")\n",
    "\n",
    "documents = bias_documents + text_documents + document_metrics\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(documents[34843])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Country: ARG\n",
      "Document Name: Laura\n",
      "Most Prevalent Bias: Racial bias' metadata={'type': 'bias type', 'Prevalence Score': 21.393770405, 'Mean Similarity Score': 0.2863502733, 'Bias Frequency': 260, 'Mean Normalized TF-IDF': 0.2873534269, 'source': '../RAG_data/document_metrics.json'}\n"
     ]
    }
   ],
   "source": [
    "print(document_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busjo\\AppData\\Local\\Temp\\ipykernel_15816\\3379331390.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/LaBSE\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstre loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/LaBSE\")\n",
    "\n",
    "index_path = \"../../Data/faiss_labse_index\"\n",
    "\n",
    "if os.path.exists(index_path):\n",
    "    vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(\"Vectorstre loaded\")\n",
    "else:\n",
    "    print(\"Creating vector store\")\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "    vectorstore.save_local(index_path)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# gpt-4o-2024-08-06\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-2024-08-06\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore.save_local(\"../faiss_labse_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the description of Confirmation bias?', 'result': 'The description of Confirmation bias is the search for and use of information to support an individual’s ideas, beliefs, or hypotheses.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the description of Confirmation bias?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the most prevalent bias from USA documents? explain why', 'result': 'The most prevalent bias from the provided USA documents is racial bias. This is indicated in two of the documents: \"AI and Society\" and \"Fairness, Ethics, Accountability, and Transparency.\" Racial bias is a significant concern in AI and societal contexts because it can lead to unfair treatment and discrimination against certain racial groups. This bias can arise from historical data that reflects existing societal inequalities, which, when used to train AI systems, can perpetuate and even exacerbate these disparities. Addressing racial bias is crucial for ensuring fairness and equity in AI applications.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the most prevalent bias from USA documents? explain why\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Why does the U.S. National AI R&D Strategic Plan stress the importance of developing AI systems that can explain their decisions?', 'result': 'The U.S. National AI R&D Strategic Plan stresses the importance of developing AI systems that can explain their decisions because it is crucial for these systems to be transparent and capable of explaining the reasons for their results to users. This transparency is essential for ensuring that AI technologies are used responsibly, safely, and beneficially, and that they align with ethical, legal, and societal principles.'}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"Why does the U.S. National AI R&D Strategic Plan stress the importance of developing AI systems that can explain their decisions?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 0 ---\n",
      "Country: USA\n",
      "Document name: National AI R&D Strategic Plan\n",
      "Sentence: Theoretical work is needed to better understand why AI techniques especially machine learning often work well in practice\n",
      "\n",
      "--- Document 1 ---\n",
      "Country: USA\n",
      "Document name: National AI R&D Strategic Plan\n",
      "Sentence: Designing a rchitecture s for e thical AI  Additional progress in fundamental research must be made to determine how to best design architectures for AI systems that incorporate ethical reasoning\n",
      "\n",
      "--- Document 2 ---\n",
      "Country: USA\n",
      "Document name: National AI R&D Strategic Plan\n",
      "Sentence: Thus, research ers must develop systems that are transparent, and intrinsically capable of explaining the reasons  for their results to users\n",
      "\n",
      "--- Document 3 ---\n",
      "Country: USA\n",
      "Document name: National AI R&D Strategic Plan\n",
      "Sentence: NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN    8 AI R&D Strategic Plan  focuses on  the R&D investments needed to help define and advance policies that ensure the responsible , safe,  and beneficial use of AI\n",
      "\n",
      "--- Document 4 ---\n",
      "Country: USA\n",
      "Document name: National AI R&D Strategic Plan\n",
      "Sentence: The recent accomplishments of AI have generated important questions on the ultimate direction and implications  of these technologies : What are the important scientific and technological g aps in current AI technologies? What new AI advances would provide positive, neede d economic and societal impacts?  How can AI technologies continue to be used safely and beneficially? How can AI systems be designed to align with ethical, legal, and societa l principles ? What are  the implications of these advancements for the AI R&D workforce?  The landscape for AI R&D is becoming increasingly complex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\busjo\\AppData\\Local\\Temp\\ipykernel_15816\\3171991013.py:1: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n--- Document {i} ---\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [\n",
    "    {\n",
    "        \"country\": \"Canada\",\n",
    "        \"language\": \"English\",\n",
    "        \"question\": \"What have Canadian Indigenous researchers and experts from around the world teamed up to publish?\",\n",
    "        \"answers\": [\n",
    "            \"ca/ai/ai-society14TH20TH24THAICan Impact Report 202022CENTRING INDIGENOUS PERSPECTIVES IN DESIGNING AIIndigenous researchers and experts from around the world have teamed up to publish Indigenous Protocol and Artificial Intelligence, a position paper32 that provides a starting place for designing ethical AI through an Indigenous-centred approach\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Denmark\",\n",
    "        \"language\": \"English\",\n",
    "        \"question\": \"What role does Carlsberg play in Denmark’s national AI strategy, and how is AI used in their beer production process?\",\n",
    "        \"answers\": [\n",
    "            \"Artificial intelligence will help beer tasting at Carlsberg1,000 different beers are screened every day at the Carlsberg laboratory in Valby near Copenhagen\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Spain\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"question\": \"What evidence does the Spanish strategy provide to demonstrate that Spain has a higher percentage of female researchers in AI compared to the EU average?\",\n",
    "        \"answers\": [\n",
    "            \"España ya cuenta con un mayor número de mujeres investigadores, el 38,8% frente al 33,8% de la UE 25, según datos del INE y Eurostat, lo que signiﬁca una buena posición para impulsar la reducción de esta brecha de género\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"European Union\",\n",
    "        \"language\": \"English\",\n",
    "        \"question\": \"What limitations does the report identify in the EU’s use of experimental policymaking, such as regulatory sandboxes?\",\n",
    "        \"answers\": [\n",
    "            \"Assessment of current initiatives of the European Commission on better regulation Third, despite the plethora of tools available to Commission services in the Toolbox, the use of experimental policymaking in the form of regulatory sandboxes and similar instruments remain orphan of a general framework at the EU level, which would allow Member States to engage in experimental policymaking\",\n",
    "            \"As it stands, Tool #69 on “Emerging Methods and Policy Instruments”, however laudable, risks providing only a theoretical opportunity for Commission policymakers, rooted in the possibility to include provisions for sandboxes in the legislative texts, rather than engaging in experimentation themselves, or relying on a structured process of experimentation in Member States while proposals are still in the making\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Italy\",\n",
    "        \"language\": \"Italian\",\n",
    "        \"question\": \"What strategy does the Italian government propose to reduce the innovation gap between northern and southern regions of the country using AI?\",\n",
    "        \"answers\": [\n",
    "            \"La rete nazionale per lIA sar inter regionale, e permetter ai territori di scambiarsi la conoscenza e le competenze per rimanere al passo dell innovazione dell IA, favorire ladozione di queste tecnologie da parte delle PMI, e quindi r idurre il divario nell'innovazione, nello sviluppo e nella competitivit delle imprese tra Nord e Sud\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Netherlands\",\n",
    "        \"language\": \"Dutch\",\n",
    "        \"question\": \"What concrete measures does the Dutch strategy propose to double the reuse of public data for AI development within five years?\",\n",
    "        \"answers\": [\n",
    "            \"30 AI VOOR NEDERLANDVERGROTEN,VERSNELLEN EN VERBINDEN 31    1,0 0,8 0,4 0,6 0,2 0,00,240,180,23NederlandVerenigde statenOostenrijkMexicoJapanFrankrijkSpanjeColombia Nieuw-ZeelandDuitslandFinlandAustralieCanadaZuid-KoreaIsraelIerland GriekenlandNoorwegen ScoreItalie   Overheid stimuleert hergebruikData BeschikbaarheidData ToegankelijkheidOm de bruikbaarheid van publieke data te verbete-ren, moet data worden opgeschoond, gelabeld en gekoppeld\",\n",
    "            \"Daarnaast moet er een duidelijk verdienmodel komen waarin de kosten voor de overheid in ver-houding zijn met de baten voor de directe gebrui-ker en de maatschappij\",\n",
    "            \"Over vijf jaar bereiken we hopelijk een verdubbeling van het hergebruik van publieke data\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"Sweden\",\n",
    "        \"language\": \"English\",\n",
    "        \"question\": \"According to the Vinnova report, what factors mutually reinforce or inhibit the development of AI-based business and operational models in Sweden?\",\n",
    "        \"answers\": [\n",
    "            \"Business and operational models, data access and competence are mutually dependent and therefore heavily affected by each other in companies and public operations\",\n",
    "            \"Without clear prospects of business benefits, the drivers of AI-based investments are inhibited\",\n",
    "            \"If the business benefit is not clear, AI competence is also not viewed as an important factor for value creation and efficiency, affecting recruitment patterns and competence development\",\n",
    "            \"Data access and possibilities of combining different data will be fundamentally significant for purposes of identifying the applications that can be developed\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"country\": \"United States\",\n",
    "        \"language\": \"English\",\n",
    "        \"question\": \"Why does the U.S. National AI R&D Strategic Plan stress the importance of developing AI systems that can explain their decisions?\",\n",
    "        \"answers\": [\n",
    "            \"First, Strategy 4 emphasizes the need for explainable and transparent systems that are trusted by their users , perform in a manner that is acceptable to the users, and can be guaranteed to act as the user intended\",\n",
    "            \"Thus, research ers must develop systems that are transparent, and intrinsically capable of explaining the reasons  for their results to users\"\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(ground_truth_data, retriever, top_k: int = 5):\n",
    "    import pandas as pd\n",
    "    results = []\n",
    "    total_hits = 0\n",
    "    total_recall = 0\n",
    "\n",
    "    for item in ground_truth_data:\n",
    "        question = item[\"question\"]\n",
    "        expected_answers = item[\"answers\"]\n",
    "        country = item[\"country\"]\n",
    "        language = item[\"language\"]\n",
    "\n",
    "        retrieved_docs = retriever.get_relevant_documents(question)\n",
    "        retrieved_texts = [doc.page_content.lower() for doc in retrieved_docs]\n",
    "        expected_texts = [ans.lower() for ans in expected_answers]\n",
    "        hit = any(\n",
    "            any(expected in retrieved for retrieved in retrieved_texts)\n",
    "            for expected in expected_texts\n",
    "        )\n",
    "        total_hits += int(hit)\n",
    "\n",
    "        relevant_found = sum(\n",
    "            any(expected in retrieved for retrieved in retrieved_texts)\n",
    "            for expected in expected_texts\n",
    "        )\n",
    "        recall = relevant_found / len(expected_texts)\n",
    "        total_recall += recall\n",
    "\n",
    "        results.append({\n",
    "            \"Country\": country,\n",
    "            \"Language\": language,\n",
    "            \"Hit Score\": 1 if hit else 0,\n",
    "            \"Recall\": round(recall, 2)\n",
    "        })\n",
    "\n",
    "    # Add average\n",
    "    avg_hit = round(total_hits / len(ground_truth_data), 2)\n",
    "    avg_recall = round(total_recall / len(ground_truth_data), 2)\n",
    "    results.append({\n",
    "        \"Country\": \"AVERAGE\",\n",
    "        \"Language\": \"\",\n",
    "        \"Hit Score\": avg_hit,\n",
    "        \"Recall\": avg_recall\n",
    "    })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df.to_string(index=False))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Country Language  Hit Score  Recall\n",
      "        Canada  English       1.00    1.00\n",
      "       Denmark  English       0.00    0.00\n",
      "         Spain  Spanish       1.00    1.00\n",
      "European Union  English       0.00    0.00\n",
      "         Italy  Italian       1.00    1.00\n",
      "   Netherlands    Dutch       1.00    0.33\n",
      "        Sweden  English       0.00    0.00\n",
      " United States  English       1.00    0.50\n",
      "       AVERAGE                0.62    0.48\n"
     ]
    }
   ],
   "source": [
    "df_results = evaluate_retriever(ground_truth, retriever, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Language</th>\n",
       "      <th>Hit Score</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canada</td>\n",
       "      <td>English</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>English</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spain</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>European Union</td>\n",
       "      <td>English</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Italian</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Dutch</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>English</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AVERAGE</td>\n",
       "      <td></td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Country Language  Hit Score  Recall\n",
       "0          Canada  English       1.00    1.00\n",
       "1         Denmark  English       0.00    0.00\n",
       "2           Spain  Spanish       1.00    1.00\n",
       "3  European Union  English       0.00    0.00\n",
       "4           Italy  Italian       1.00    1.00\n",
       "5     Netherlands    Dutch       1.00    0.33\n",
       "6          Sweden  English       0.00    0.00\n",
       "7   United States  English       1.00    0.50\n",
       "8         AVERAGE                0.62    0.48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
