{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5647 documents.\n",
      "page_content='Country: USA\n",
      "Document number: 2\n",
      "Most Prevalent Bias: Confirmation bias' metadata={'type': 'bias type', 'Prevalence Score': 0.4346623644, 'Mean Similarity Score': 0.2948431373, 'Bias Frequency': 5, 'Mean BLEU Score': 1.2374582985, 'source': 'RAG_data/document_metrics.json'}\n"
     ]
    }
   ],
   "source": [
    "def load_bias_terms(file_path: str) -> list[Document]:\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        bias_type = entry.get(\"Bias_Type\", \"Unknown Bias\")\n",
    "        description = entry.get(\"Description\", \"\")\n",
    "        content = f\"Bias: {bias_type}\\nDescription: {description}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"definition\", \"source\": file_path}))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_document_text(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_nr = entry.get(\"Document_nr\", \"\")\n",
    "        text_chunk = entry.get(\"Text\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument number: {doc_nr}\\nSentence: {text_chunk}\"\n",
    "        documents.append(Document(page_content=content, metadata={\"type\":\"text\", \"source\": file_path}))\n",
    "    return documents\n",
    "\n",
    "def load_document_metrics(file_path: str) -> list[Document]:\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    documents = []\n",
    "    for entry in data:\n",
    "        country = entry.get(\"Country\", \"Unknown country\")\n",
    "        doc_nr = entry.get(\"Document_nr\", \"\")\n",
    "        bias_type = entry.get(\"Most Prevalent Bias\", \"\")\n",
    "        content = f\"Country: {country}\\nDocument number: {doc_nr}\\nMost Prevalent Bias: {bias_type}\"\n",
    "\n",
    "        prevalence_score = entry.get(\"Prevalence Score\", \"\")\n",
    "        mean_similarity_score = entry.get(\"Mean Similarity Score\", \"\")\n",
    "        bias_frequency = entry.get(\"Bias Frequency\", \"\")\n",
    "        mean_BLEU_score = entry.get(\"Mean BLEU Score\", \"\")\n",
    "        metadata = {\"type\":\"bias type\", \"Prevalence Score\":prevalence_score, \"Mean Similarity Score\":mean_similarity_score, \"Bias Frequency\":bias_frequency, \n",
    "                    \"Mean BLEU Score\":mean_BLEU_score, \"source\": file_path}\n",
    "\n",
    "        documents.append(Document(page_content=content, metadata=metadata))\n",
    "    return documents\n",
    "\n",
    "\n",
    "bias_documents = load_bias_terms(\"RAG_data/bias_terms.json\")\n",
    "text_documents = load_document_text(\"RAG_data/documents_text.json\")\n",
    "document_metrics = load_document_metrics(\"RAG_data/document_metrics.json\")\n",
    "\n",
    "documents = bias_documents + text_documents + document_metrics\n",
    "print(f\"Loaded {len(documents)} documents.\")\n",
    "print(documents[5643])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the description of Confirmation bias?', 'result': 'Confirmation bias is the search for and use of information to support an individualâ€™s ideas, beliefs, or hypotheses.'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI  # Updated import\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# Create a FAISS vector store from the documents\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# Create a retriever with the top-3 most similar documents\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize your LLM using the updated ChatOpenAI for chat-based models\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# Build the RetrievalQA (RAG) chain\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the description of Confirmation bias?\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is the most prevalent bias from USA documents? Explain why', 'result': \"The most prevalent bias in USA documents varies depending on the specific document. Document number 3 shows information bias, document number 5 shows reporting biases, and document number 2 shows confirmation bias. Each bias has different characteristics and implications. Information bias occurs when there is a systematic error in the way data is collected, leading to incorrect conclusions. Reporting biases involve selective reporting of information, which can skew the overall picture presented. Confirmation bias is the tendency to search for, interpret, or remember information in a way that confirms one's preconceptions. Each bias can impact the accuracy and reliability of the information presented in the documents.\"}\n"
     ]
    }
   ],
   "source": [
    "# Query the RAG system using the invoke method\n",
    "query = \"What is the most prevalent bias from USA documents? Explain why\"\n",
    "result = qa_chain.invoke(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer (using the 'bert-base-uncased' model)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# A simple function to load a JSON file and convert it to a LangChain Document.\n",
    "def load_json_to_document(file_path: str) -> Document:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # Assume the JSON contains a field \"text\". If not, convert the whole JSON to a string.\n",
    "    text = data.get(\"text\", str(data))\n",
    "    return Document(page_content=text, metadata={\"source\": file_path})\n",
    "\n",
    "# List your JSON file paths\n",
    "doc_paths = [\"doc1.json\", \"doc2.json\", \"doc3.json\"]\n",
    "\n",
    "# Load documents from the JSON files\n",
    "documents = [load_json_to_document(path) for path in doc_paths]\n",
    "\n",
    "# Optional: Function to tokenize a document with BERT and split into smaller chunks if needed.\n",
    "def tokenize_document(doc: Document, max_tokens: int = 128):\n",
    "    tokens = tokenizer.tokenize(doc.page_content)\n",
    "    # If the document is longer than max_tokens, split it into chunks.\n",
    "    if len(tokens) > max_tokens:\n",
    "        chunks = []\n",
    "        for i in range(0, len(tokens), max_tokens):\n",
    "            chunk_tokens = tokens[i:i+max_tokens]\n",
    "            chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "            chunks.append(Document(page_content=chunk_text, metadata=doc.metadata))\n",
    "        return chunks\n",
    "    else:\n",
    "        return [doc]\n",
    "\n",
    "# Process each document (this step is optional if you don't require chunking)\n",
    "processed_docs = []\n",
    "for doc in documents:\n",
    "    processed_docs.extend(tokenize_document(doc))\n",
    "\n",
    "# Initialize HuggingFace embeddings (you can choose a model based on your needs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a FAISS vector store from the processed documents\n",
    "vectorstore = FAISS.from_documents(processed_docs, embeddings)\n",
    "\n",
    "# Convert the vectorstore into a retriever (here, returning the top-3 nearest neighbors)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize your LLM (OpenAI's model in this case; ensure your API key is set in your environment)\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Create the RetrievalQA (RAG) chain tying the retriever and the LLM together\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# Query the RAG system\n",
    "query = \"What are the key points discussed in the documents?\"\n",
    "result = qa_chain.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected page_content is string, got <class 'dict'> instead.                     Set `text_content=False` if the desired input for                     `page_content` is not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m document_metrics_loader \u001b[38;5;241m=\u001b[39m JSONLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG_data/document_metrics.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, jq_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m documents_text_loader \u001b[38;5;241m=\u001b[39m JSONLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG_data/documents_text.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, jq_schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m bias_terms \u001b[38;5;241m=\u001b[39m \u001b[43mbias_terms_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#document_metrics = document_metrics_loader.load()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#documents_text = documents_text_loader.load()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Combine the documents into one list\u001b[39;00m\n\u001b[0;32m     10\u001b[0m documents \u001b[38;5;241m=\u001b[39m bias_terms \u001b[38;5;241m+\u001b[39m document_metrics \u001b[38;5;241m+\u001b[39m documents_text\n",
      "File \u001b[1;32mc:\\Users\\busjo\\Documents\\JADS\\Thesis\\AI_Policy_Thesis\\venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\busjo\\Documents\\JADS\\Thesis\\AI_Policy_Thesis\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:147\u001b[0m, in \u001b[0;36mJSONLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m                     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mc:\\Users\\busjo\\Documents\\JADS\\Thesis\\AI_Policy_Thesis\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:162\u001b[0m, in \u001b[0;36mJSONLoader._parse\u001b[1;34m(self, content, index)\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_content_key(data)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data, index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata(\n\u001b[0;32m    164\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path), seq_num\u001b[38;5;241m=\u001b[39mi\n\u001b[0;32m    165\u001b[0m     )\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[1;32mc:\\Users\\busjo\\Documents\\JADS\\Thesis\\AI_Policy_Thesis\\venv\\Lib\\site-packages\\langchain_community\\document_loaders\\json_loader.py:180\u001b[0m, in \u001b[0;36mJSONLoader._get_text\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    177\u001b[0m     content \u001b[38;5;241m=\u001b[39m sample\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_text_content \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected page_content is string, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124m            Set `text_content=False` if the desired input for \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124m            `page_content` is not a string\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    184\u001b[0m     )\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# In case the text is None, set it to an empty string\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: Expected page_content is string, got <class 'dict'> instead.                     Set `text_content=False` if the desired input for                     `page_content` is not a string"
     ]
    }
   ],
   "source": [
    "bias_terms_loader = JSONLoader(\"RAG_data/bias_terms.json\", jq_schema=\".[] | {page_content: .Description, metadata: {Bias_Type: .Bias_Type}}\")\n",
    "document_metrics_loader = JSONLoader(\"RAG_data/document_metrics.json\", jq_schema=\".\")\n",
    "documents_text_loader = JSONLoader(\"RAG_data/documents_text.json\", jq_schema=\".\")\n",
    "\n",
    "bias_terms = bias_terms_loader.load()\n",
    "#document_metrics = document_metrics_loader.load()\n",
    "#documents_text = documents_text_loader.load()\n",
    "\n",
    "# Combine the documents into one list\n",
    "documents = bias_terms + document_metrics + documents_text\n",
    "\n",
    "# Initialize HuggingFaceEmbeddings with a BERT model.\n",
    "# This will automatically use the BERT tokenizer from Hugging Face.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")\n",
    "\n",
    "# Build a FAISS vectorstore from the documents\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Initialize your LLM and set up the RetrievalQA chain\n",
    "llm = OpenAI(temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "# Run a query\n",
    "query = \"What are the most important types biases discussed in the USA policy documents\"\n",
    "result = qa_chain.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
