{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "pd.set_option('display.max_colwidth', None)  # Show full column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Bias_Type': 'Adherence bias', 'Description': 'A systematic distortion in outcome data that arises when participants who adhere to a study protocol or intervention differ from those who do not adhere, when that difference relates to the outcome of interest.'}, {'Bias_Type': 'Admission rate bias', 'Description': 'Arises when the variables under study are affected by the selection of hospitalized subjects leading to a bias between the exposure and the\\xa0disease under study.'}, {'Bias_Type': 'All’s well literature bias', 'Description': 'Occurs when publications omit or play down controversies or disparate results.'}, {'Bias_Type': 'Allocation bias', 'Description': 'Systematic difference in how participants are assigned to comparison groups in a clinical trial.'}, {'Bias_Type': 'Apprehension bias', 'Description': 'When a study participant responds differently due to being observed'}, {'Bias_Type': 'Ascertainment bias', 'Description': 'Systematic differences in the identification of individuals included in a study or distortion in the collection of data in a study.'}, {'Bias_Type': 'Attrition bias', 'Description': 'Unequal loss of participants from study groups in a trial.'}, {'Bias_Type': 'Availability bias', 'Description': 'A distortion that arises from the use of information which is most readily available, rather than that which is necessarily most representative.'}, {'Bias_Type': 'Biases of rhetoric', 'Description': 'An argument used to persuade the reader without appealing to reason or evidence.'}, {'Bias_Type': 'Centripetal bias', 'Description': 'The reputations of certain clinicians and institutions cause individuals with specific disorders or exposures to gravitate toward them.'}, {'Bias_Type': 'Chronological bias', 'Description': 'When study participants allocated earlier to an intervention or a group are subject to different exposures or are at a different risk from participants who are recruited later.'}, {'Bias_Type': 'Collider bias', 'Description': 'A distortion that modifies an association between an exposure and outcome, caused by attempts to control for a common effect of the exposure and outcome'}, {'Bias_Type': 'Compliance bias', 'Description': 'Participants compliant with an intervention differ in some way from those not compliant which can systematically affect the outcome of interest.'}, {'Bias_Type': 'Confirmation bias', 'Description': 'The search for and use of information to support an individual’s ideas, beliefs or hypotheses.'}, {'Bias_Type': 'Confounding', 'Description': 'A distortion that modifies an association between an exposure and an outcome\\xa0because a factor is independently associated with the exposure and the outcome.'}, {'Bias_Type': 'Confounding by indication', 'Description': 'A distortion that modifies an association between an exposure and an outcome, caused by the presence of an indication for the exposure that is the true cause of the outcome.'}, {'Bias_Type': 'Data-dredging bias', 'Description': 'A distortion that arises from presenting the results of unplanned statistical tests as if they were a fully prespecified course of analyses.'}, {'Bias_Type': 'Detection bias', 'Description': 'Systematic differences between groups in how outcomes are determined.'}, {'Bias_Type': 'Diagnostic access bias', 'Description': 'Individuals differ in their geographic, temporal and economic access to diagnostic procedures which label them as having a given disease.'}, {'Bias_Type': 'Diagnostic suspicion bias', 'Description': 'Knowledge of a subject’s prior exposures or personal biases may influence both the process and the outcome of diagnostic tests.'}, {'Bias_Type': 'Differential Reference bias', 'Description': 'When not all participants receive the same reference test in a diagnostic accuracy study.'}, {'Bias_Type': 'Hawthorne effect', 'Description': 'When individuals modify an aspect of their behaviour in response to their awareness of being observed.'}, {'Bias_Type': 'Hot stuff bias', 'Description': 'When a topic is fashionable (‘hot’)\\xa0 investigators may be less critical in their approach to their research, and investigators and editors may not be able to resist the temptation to publish the results.'}, {'Bias_Type': 'Hypothetical bias', 'Description': 'A distortion that arises when an individual’s stated behaviour or valuation differs to that of their real behaviour or valuation.'}, {'Bias_Type': 'Immortal time bias', 'Description': 'A distortion that modifies an association between an exposure and an outcome, caused when a cohort study is designed so that follow-up includes a period of time where participants in the exposed group cannot experience the outcome and are essentially ‘immortal’.'}, {'Bias_Type': 'Incorporation bias', 'Description': 'When the results of an index test form part of the reference test in a diagnostic study.'}, {'Bias_Type': 'Industry Sponsorship bias', 'Description': 'A tendency for the methods and results of a study to support the interests of the funding organisation.'}, {'Bias_Type': 'Information bias', 'Description': 'Bias that arises from systematic differences in the collection, recall, recording or handling of information used in a study.'}, {'Bias_Type': 'Informed presence bias', 'Description': 'The presence of a person’s information in an electronic health record is affected by the person’s health status.'}, {'Bias_Type': 'Insensitive measure bias', 'Description': 'The use of an insufficiently accurate method to detect the outcome of interest, such that clinically important differences are not detected.'}, {'Bias_Type': 'Lack of blinding', 'Description': 'The lack of concealment of an intervention or control treatment received by participants in a clinical trial.'}, {'Bias_Type': 'Language bias', 'Description': 'Publication of research findings in a particular language.'}, {'Bias_Type': 'Lead time bias', 'Description': 'A distortion overestimating the apparent time surviving with a disease caused by bringing forward the time of its diagnosis'}, {'Bias_Type': 'Mimicry bias', 'Description': 'An innocent exposure may become suspicious if, rather than causing disease, it causes a benign disorder which resembles the disease.'}, {'Bias_Type': 'Misclassification bias', 'Description': 'Occurs when a study participant is categorised into an incorrect category altering the observed association or research outcome of interest.'}, {'Bias_Type': 'Non-contemporaneous control bias', 'Description': 'Differences in the timing of selection of case and controls within in a study influence exposures and outcomes resulting in biased estimates.'}, {'Bias_Type': 'Non-response bias', 'Description': 'A bias that occurs due to systematic differences between responders and non-responders'}, {'Bias_Type': 'Novelty bias', 'Description': 'The tendency for an intervention to appear better when it is new.'}, {'Bias_Type': 'Observer bias', 'Description': 'The process of observing and recording information which includes systematic discrepancies from the truth.'}, {'Bias_Type': 'One-sided reference bias', 'Description': 'When authors restrict their references to only those works that support their position.'}, {'Bias_Type': 'Outcome reporting bias', 'Description': 'The selective reporting of pre-specified outcomes in published clinical trials.'}, {'Bias_Type': 'Partial reference bias', 'Description': 'When only a proportion of the study group receive both the index and reference standard test when investigating for diagnostic accuracy.'}, {'Bias_Type': 'Perception bias', 'Description': 'The tendency to be subjective about people and events, causing biased information to be collected in a study or biased interpretation of a study’s results.'}, {'Bias_Type': 'Performance bias', 'Description': 'Systematic differences in the care provided to members of different study groups other than the intervention under investigation'}, {'Bias_Type': 'Popularity bias', 'Description': 'Differences in the uptake of\\xa0healthcare as a result of a public interest in a disease or condition and its possible causes results in a biased study sample'}, {'Bias_Type': 'Positive results bias', 'Description': 'The tendency to submit, accept and publish positive results\\xa0rather than non-significant or negative results.'}, {'Bias_Type': 'Prevalence-incidence (Neyman) bias', 'Description': 'Exclusion of individuals with severe or mild disease resulting in a\\xa0systematic error in the estimated association or effect of an exposure on an outcome.'}, {'Bias_Type': 'Previous opinion bias', 'Description': 'The results of a previous assessment, test result\\xa0or diagnosis, if known, may affect the results of subsequent processes on the same patient.'}, {'Bias_Type': 'Publication bias', 'Description': 'When the likelihood of a study being published is affected by the findings of the study.'}, {'Bias_Type': 'Racial bias', 'Description': 'A distortion arising from systemic, institutional, interpersonal or individual forms of explicit (conscious) or implicit (unconscious) prejudice against individuals or groups based on social constructs of race or ethnicity that influences the planning, methods, results, interpretation, dissemination and application of health research.'}, {'Bias_Type': 'Recall bias', 'Description': 'Systematic error due to differences in accuracy or completeness of recall to memory of past events or experiences.'}, {'Bias_Type': 'Referral filter bias', 'Description': 'Referral of any group of unwell people from primary to secondary to tertiary care, causing an increase in the concentration of rare cases, more complex cases or people with worse outcomes.'}, {'Bias_Type': 'Reporting biases', 'Description': 'A systematic distortion that arises from the selective disclosure or withholding of information by parties involved in the design, conduct, analysis, or dissemination of a study or research findings'}, {'Bias_Type': 'Review biases', 'Description': 'Occurs in diagnostic test accuracy studies when the person interpreting the results of the index test has knowledge of the results of the reference standard (diagnostic review bias), or, the person interpreting the results of the reference standard\\xa0has knowledge of the results of the index test (index review bias). Clinical review bias occurs when relevant clinical and patient information is available to the person interpreting the test or reference result. These are all forms of observer bias.'}, {'Bias_Type': 'Selection bias', 'Description': 'occurs when individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome.'}, {'Bias_Type': 'Spectrum bias', 'Description': 'Occurs when a diagnostic test is studied in a different range of individuals to the intended population for the test'}, {'Bias_Type': 'Spin bias', 'Description': 'The intentional or unintentional distorted interpretation of research results, unjustifiably suggesting favourable or unfavourable findings that can result in misleading conclusions'}, {'Bias_Type': 'Starting time bias', 'Description': 'Arises when there is a failure to identify a common starting time for an exposure or a disease.'}, {'Bias_Type': 'Substitution game bias', 'Description': 'Substitution of the clinically important\\xa0endpoint, or an exposure, with a surrogate marker for the disease.'}, {'Bias_Type': 'Unacceptability bias', 'Description': 'A systematic difference in response rates or uptake of tests due to their “unacceptability”'}, {'Bias_Type': 'Unacceptable disease bias', 'Description': 'Lower rates of reporting of certain “unacceptable” diseases compared with other health conditions.'}, {'Bias_Type': 'Unmasking (detection signal) bias', 'Description': 'An innocent exposure that, rather than causing a disease, causes a sign or symptom that precipitates a search for the disease.'}, {'Bias_Type': 'Verification bias', 'Description': 'when only a proportion of the study group receives confirmation of the diagnosis by the reference standard, or if some patients receive a different reference standard at the time of diagnosis.'}, {'Bias_Type': 'Volunteer bias', 'Description': 'Participants volunteering to take part in a study intrinsically have different characteristics from the general population of interest.'}, {'Bias_Type': 'Wrong sample size bias', 'Description': 'When the wrong sample size is used in a study: small sample sizes often lead to chance findings, while large sample sizes are often statistically significant but\\xa0not clinically relevant.'}]\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/bias_terms.json', 'r') as file:\n",
    "    bias_json_data = json.load(file)\n",
    "print(bias_json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"NATIONAL SCIENCE FOUNDATION2415 EISENHOWER AVENUEALEXANDRIA, VIRGINIA 22314NSF 19-018Dear Colleague Letter: EArly-concept Grants for ExploratoryResearch on Artificial Intelligence (AI) and Society - SupportedJointly with the Partnership on AINovember 1 5, 2018Dear Colleagues:The National Science Foundation's (NSF) Directorates for Computer and Information Scienceand Engineering (CISE) and Social, Behavioral and Economic Sciences (SBE) together withthe Partnership on AI (PAI) wish to notify the community of their interest in supporting EArly-concept Grants for Exploratory Research (EAGERs) to understand the social challengesarising from AI technology and enable scientific contributions to overcome them\",\n",
       " 'The last 20 years have seen rapid advances in machine learning, pattern recognition,planning, effective decision making, natural language processing, and machine vision',\n",
       " 'Theseadvances have been fueled by increased data, faster computation, and improved algorithms',\n",
       " 'They are yielding increasingly diverse and large-scale applications deployed in settingssubject to unanticipated challenges with complex social effects',\n",
       " 'NSF has long supported fundamental research enabling AI technology',\n",
       " 'With increases in thescale and diversity of deployments comes the need to better understand AI in the open world,including unforeseen circumstances and social impacts, and to craft approaches to AI thatconsider these from the start',\n",
       " 'Vital directions include developing principles for safe, robust,and trustworthy AI (including shared responsibilities between humans and AI systems);addressing issues of bias, fairness, and transparency of algorithmic intelligence; developingdeeper understanding of human-AI interaction and user education; and developing insightsabout the influences of AI on people and society',\n",
       " 'NSF and PAI will jointly support high-risk, high-reward research at the intersection of thesocial and technical dimensions of AI',\n",
       " 'Priority will be given to collaborative projects thatintegrate computer/computational science with the social, behavioral, and economicsciences',\n",
       " 'Proposals may expand understanding of the influences of AI on people andsociety or contribute technical innovations that overcome the emerging social challenges',\n",
       " '1This document has been archived',\n",
       " 'Topics include, but are not limited to:Safety, robustness, and accountability of AI systems;Bias and fairness of AI systems;Intelligibility, explanation, and transparency of AI inferences;Privacy challenges with AI development and use;Sociotechnical challenges involving ethical considerations;Economic impacts of AI on society; andSocial consequences of AI system deployments',\n",
       " 'EAGER proposals that fail to address concepts described in this DCL will be returned withoutreview',\n",
       " 'An individual may appear as principal investigator (PI), co-PI, Senior Personnel, orConsultant on no more than one  EAGER proposal submitted in response to this DCL',\n",
       " 'Community response to this DCL will help identify ground-breaking directions',\n",
       " 'In parallel, theComputing Community Consortium (CCC)  is leading an AI \"roadmapping\" effort that willcomplement the EAGERs funded pursuant to this DCL',\n",
       " 'That roadmapping activity aims tobuild community consensus around grand challenges in AI and an associated long-term andinterdisciplinary research agenda',\n",
       " 'EAGER proposals pursuant to this DCL are welcome through February 28, 2019 , but earlier submissions are encouraged',\n",
       " 'Submissions should follow the guidance in NSF Proposal & Award Policies & Procedures Guide  (PAPPG) Chapter II',\n",
       " 'NSF and the Partnership on AI anticipate jointly supporting up to 15 EAGER awards, each up to $300,000  for up to two years, in accordance with the PAPPG',\n",
       " 'EAGER proposals pursuant to this DCL  must include the prefix \"AI-DCL:\" in front of the title',\n",
       " 'Once NSF program officers have approved the prospectus, the PIwill be invited to submit a full EAGER proposal to a specific program',\n",
       " 'The prospectus shouldbe responsive to the DCL and make a compelling case that the project is suitable for anEAGER',\n",
       " 'The prospectus must be received no later than January 9, 2019',\n",
       " 'Sincerely,Jim KuroseAssistant Director, CISE2Arthur LupiaAssistant Director, SBE 3',\n",
       " 'To reduce the use of information that merely supports an individual’s ideas, AI models must be trained on datasets encompassing diverse and potentially contradictory viewpoints to mitigate confirmation bias.',\n",
       " 'AI systems must prioritize the relevance and quality of the data used over mere volume, to prevent bias arising from differences in the collection of information',\n",
       " 'To address distortions caused by prejudice against racial or ethnic groups, developers must audit training datasets to ensure no group is underrepresented or mischaracterized, thereby reducing racial bias in AI outcomes.',\n",
       " 'To mitigate selection bias, which occurs when sampled groups differ systematically from the target population, training datasets must be representative of the diversity within the actual end-user base.',\n",
       " 'AI policy requires full transparency in documenting dataset sources and limitations, to prevent bias caused by selective reporting or omission of information.']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../Data/Policy_docs/USA/AI and Society/AI and Society.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "policy_text = text\n",
    "policy_text = policy_text.replace('\\n', '')\n",
    "\n",
    "policy_text = re.sub(r'\\b(?:https?://|www\\.)\\S+\\b', '', policy_text)\n",
    "\n",
    "policy_sentences = policy_text.split('.')\n",
    "policy_sentences = [s.strip() for s in policy_sentences if s.strip()]\n",
    "policy_sentences = [s for s in policy_sentences if '@' not in s]\n",
    "policy_sentences = [s for s in policy_sentences if len(s) >= 10]\n",
    "policy_sentences = [s for s in policy_sentences if 'et al' not in s]\n",
    "\n",
    "bias_sentences = [\n",
    "    \"To reduce the use of information that merely supports an individual’s ideas, AI models must be trained on datasets encompassing diverse and potentially contradictory viewpoints to mitigate confirmation bias.\",\n",
    "    \n",
    "    \"AI systems must prioritize the relevance and quality of the data used over mere volume, to prevent bias arising from differences in the collection of information\",\n",
    "    \n",
    "    \"To address distortions caused by prejudice against racial or ethnic groups, developers must audit training datasets to ensure no group is underrepresented or mischaracterized, thereby reducing racial bias in AI outcomes.\",\n",
    "    \n",
    "    \"To mitigate selection bias, which occurs when sampled groups differ systematically from the target population, training datasets must be representative of the diversity within the actual end-user base.\",\n",
    "    \n",
    "    \"AI policy requires full transparency in documenting dataset sources and limitations, to prevent bias caused by selective reporting or omission of information.\"\n",
    "]\n",
    "\n",
    "[policy_sentences.append(sentence) for sentence in bias_sentences]\n",
    "policy_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adherence bias A systematic distortion in outcome data that arises when participants who adhere to a study protocol or intervention differ from those who do not adhere, when that difference relates to the outcome of interest.', 'Admission rate bias Arises when the variables under study are affected by the selection of hospitalized subjects leading to a bias between the exposure and the\\xa0disease under study.', 'All’s well literature bias Occurs when publications omit or play down controversies or disparate results.', 'Allocation bias Systematic difference in how participants are assigned to comparison groups in a clinical trial.', 'Apprehension bias When a study participant responds differently due to being observed', 'Ascertainment bias Systematic differences in the identification of individuals included in a study or distortion in the collection of data in a study.', 'Attrition bias Unequal loss of participants from study groups in a trial.', 'Availability bias A distortion that arises from the use of information which is most readily available, rather than that which is necessarily most representative.', 'Biases of rhetoric An argument used to persuade the reader without appealing to reason or evidence.', 'Centripetal bias The reputations of certain clinicians and institutions cause individuals with specific disorders or exposures to gravitate toward them.', 'Chronological bias When study participants allocated earlier to an intervention or a group are subject to different exposures or are at a different risk from participants who are recruited later.', 'Collider bias A distortion that modifies an association between an exposure and outcome, caused by attempts to control for a common effect of the exposure and outcome', 'Compliance bias Participants compliant with an intervention differ in some way from those not compliant which can systematically affect the outcome of interest.', 'Confirmation bias The search for and use of information to support an individual’s ideas, beliefs or hypotheses.', 'Confounding A distortion that modifies an association between an exposure and an outcome\\xa0because a factor is independently associated with the exposure and the outcome.', 'Confounding by indication A distortion that modifies an association between an exposure and an outcome, caused by the presence of an indication for the exposure that is the true cause of the outcome.', 'Data-dredging bias A distortion that arises from presenting the results of unplanned statistical tests as if they were a fully prespecified course of analyses.', 'Detection bias Systematic differences between groups in how outcomes are determined.', 'Diagnostic access bias Individuals differ in their geographic, temporal and economic access to diagnostic procedures which label them as having a given disease.', 'Diagnostic suspicion bias Knowledge of a subject’s prior exposures or personal biases may influence both the process and the outcome of diagnostic tests.', 'Differential Reference bias When not all participants receive the same reference test in a diagnostic accuracy study.', 'Hawthorne effect When individuals modify an aspect of their behaviour in response to their awareness of being observed.', 'Hot stuff bias When a topic is fashionable (‘hot’)\\xa0 investigators may be less critical in their approach to their research, and investigators and editors may not be able to resist the temptation to publish the results.', 'Hypothetical bias A distortion that arises when an individual’s stated behaviour or valuation differs to that of their real behaviour or valuation.', 'Immortal time bias A distortion that modifies an association between an exposure and an outcome, caused when a cohort study is designed so that follow-up includes a period of time where participants in the exposed group cannot experience the outcome and are essentially ‘immortal’.', 'Incorporation bias When the results of an index test form part of the reference test in a diagnostic study.', 'Industry Sponsorship bias A tendency for the methods and results of a study to support the interests of the funding organisation.', 'Information bias Bias that arises from systematic differences in the collection, recall, recording or handling of information used in a study.', 'Informed presence bias The presence of a person’s information in an electronic health record is affected by the person’s health status.', 'Insensitive measure bias The use of an insufficiently accurate method to detect the outcome of interest, such that clinically important differences are not detected.', 'Lack of blinding The lack of concealment of an intervention or control treatment received by participants in a clinical trial.', 'Language bias Publication of research findings in a particular language.', 'Lead time bias A distortion overestimating the apparent time surviving with a disease caused by bringing forward the time of its diagnosis', 'Mimicry bias An innocent exposure may become suspicious if, rather than causing disease, it causes a benign disorder which resembles the disease.', 'Misclassification bias Occurs when a study participant is categorised into an incorrect category altering the observed association or research outcome of interest.', 'Non-contemporaneous control bias Differences in the timing of selection of case and controls within in a study influence exposures and outcomes resulting in biased estimates.', 'Non-response bias A bias that occurs due to systematic differences between responders and non-responders', 'Novelty bias The tendency for an intervention to appear better when it is new.', 'Observer bias The process of observing and recording information which includes systematic discrepancies from the truth.', 'One-sided reference bias When authors restrict their references to only those works that support their position.', 'Outcome reporting bias The selective reporting of pre-specified outcomes in published clinical trials.', 'Partial reference bias When only a proportion of the study group receive both the index and reference standard test when investigating for diagnostic accuracy.', 'Perception bias The tendency to be subjective about people and events, causing biased information to be collected in a study or biased interpretation of a study’s results.', 'Performance bias Systematic differences in the care provided to members of different study groups other than the intervention under investigation', 'Popularity bias Differences in the uptake of\\xa0healthcare as a result of a public interest in a disease or condition and its possible causes results in a biased study sample', 'Positive results bias The tendency to submit, accept and publish positive results\\xa0rather than non-significant or negative results.', 'Prevalence-incidence (Neyman) bias Exclusion of individuals with severe or mild disease resulting in a\\xa0systematic error in the estimated association or effect of an exposure on an outcome.', 'Previous opinion bias The results of a previous assessment, test result\\xa0or diagnosis, if known, may affect the results of subsequent processes on the same patient.', 'Publication bias When the likelihood of a study being published is affected by the findings of the study.', 'Racial bias A distortion arising from systemic, institutional, interpersonal or individual forms of explicit (conscious) or implicit (unconscious) prejudice against individuals or groups based on social constructs of race or ethnicity that influences the planning, methods, results, interpretation, dissemination and application of health research.', 'Recall bias Systematic error due to differences in accuracy or completeness of recall to memory of past events or experiences.', 'Referral filter bias Referral of any group of unwell people from primary to secondary to tertiary care, causing an increase in the concentration of rare cases, more complex cases or people with worse outcomes.', 'Reporting biases A systematic distortion that arises from the selective disclosure or withholding of information by parties involved in the design, conduct, analysis, or dissemination of a study or research findings', 'Review biases Occurs in diagnostic test accuracy studies when the person interpreting the results of the index test has knowledge of the results of the reference standard (diagnostic review bias), or, the person interpreting the results of the reference standard\\xa0has knowledge of the results of the index test (index review bias). Clinical review bias occurs when relevant clinical and patient information is available to the person interpreting the test or reference result. These are all forms of observer bias.', 'Selection bias occurs when individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome.', 'Spectrum bias Occurs when a diagnostic test is studied in a different range of individuals to the intended population for the test', 'Spin bias The intentional or unintentional distorted interpretation of research results, unjustifiably suggesting favourable or unfavourable findings that can result in misleading conclusions', 'Starting time bias Arises when there is a failure to identify a common starting time for an exposure or a disease.', 'Substitution game bias Substitution of the clinically important\\xa0endpoint, or an exposure, with a surrogate marker for the disease.', 'Unacceptability bias A systematic difference in response rates or uptake of tests due to their “unacceptability”', 'Unacceptable disease bias Lower rates of reporting of certain “unacceptable” diseases compared with other health conditions.', 'Unmasking (detection signal) bias An innocent exposure that, rather than causing a disease, causes a sign or symptom that precipitates a search for the disease.', 'Verification bias when only a proportion of the study group receives confirmation of the diagnosis by the reference standard, or if some patients receive a different reference standard at the time of diagnosis.', 'Volunteer bias Participants volunteering to take part in a study intrinsically have different characteristics from the general population of interest.', 'Wrong sample size bias When the wrong sample size is used in a study: small sample sizes often lead to chance findings, while large sample sizes are often statistically significant but\\xa0not clinically relevant.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "docs = [\n",
    "    Document(page_content=item[\"Bias_Type\"] + \" \" + item[\"Description\"], metadata={\"bias\": item[\"Bias_Type\"]})\n",
    "    for item in bias_json_data\n",
    "]\n",
    "docs_text = [doc.page_content for doc in docs]\n",
    "print(docs_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "docs = [\n",
    "    Document(page_content=item[\"Bias_Type\"] + \" \" + item[\"Description\"], metadata={\"bias\": item[\"Bias_Type\"]})\n",
    "    for item in bias_json_data\n",
    "]\n",
    "docs_text = [doc.page_content for doc in docs]\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "\n",
    "raw_embeddings = model.encode(docs_text, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "embedding_dim = raw_embeddings.shape[1]\n",
    "\n",
    "# Create a FAISS index that uses inner product (which, for normalized vectors, equals cosine similarity)\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "index.add(raw_embeddings)\n",
    "\n",
    "docstore = {i: doc for i, doc in enumerate(docs)}\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Perform similarity search on policy sentences\n",
    "for chunk in policy_sentences:\n",
    "    # Compute the raw query embedding\n",
    "    query_emb = model.encode([chunk], normalize_embeddings=True).astype(\"float32\")\n",
    "    \n",
    "    # Perform search in the FAISS index\n",
    "    distances, indices = index.search(query_emb, k=1)\n",
    "    best_idx = indices[0][0]\n",
    "    best_score = distances[0][0] \n",
    "    \n",
    "    best_doc = docstore[best_idx]\n",
    "\n",
    "    results_list.append({\n",
    "        \"Text Chunk\": chunk,\n",
    "        \"Best Matching Bias\": best_doc.metadata[\"bias\"],\n",
    "        \"Similarity Score\": best_score\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text Chunk</th>\n",
       "      <th>Best Matching Bias</th>\n",
       "      <th>Similarity Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NATIONAL SCIENCE FOUNDATION2415 EISENHOWER AVENUEALEXANDRIA, VIRGINIA 22314NSF 19-018Dear Colleague Letter: EArly-concept Grants for ExploratoryResearch on Artificial Intelligence (AI) and Society - SupportedJointly with the Partnership on AINovember 1 5, 2018Dear Colleagues:The National Science Foundation's (NSF) Directorates for Computer and Information Scienceand Engineering (CISE) and Social, Behavioral and Economic Sciences (SBE) together withthe Partnership on AI (PAI) wish to notify the community of their interest in supporting EArly-concept Grants for Exploratory Research (EAGERs) to understand the social challengesarising from AI technology and enable scientific contributions to overcome them</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.280374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The last 20 years have seen rapid advances in machine learning, pattern recognition,planning, effective decision making, natural language processing, and machine vision</td>\n",
       "      <td>Information bias</td>\n",
       "      <td>0.304363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Theseadvances have been fueled by increased data, faster computation, and improved algorithms</td>\n",
       "      <td>Information bias</td>\n",
       "      <td>0.300446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are yielding increasingly diverse and large-scale applications deployed in settingssubject to unanticipated challenges with complex social effects</td>\n",
       "      <td>Racial bias</td>\n",
       "      <td>0.366127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NSF has long supported fundamental research enabling AI technology</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.339133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With increases in thescale and diversity of deployments comes the need to better understand AI in the open world,including unforeseen circumstances and social impacts, and to craft approaches to AI thatconsider these from the start</td>\n",
       "      <td>Racial bias</td>\n",
       "      <td>0.363890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vital directions include developing principles for safe, robust,and trustworthy AI (including shared responsibilities between humans and AI systems);addressing issues of bias, fairness, and transparency of algorithmic intelligence; developingdeeper understanding of human-AI interaction and user education; and developing insightsabout the influences of AI on people and society</td>\n",
       "      <td>Racial bias</td>\n",
       "      <td>0.450619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NSF and PAI will jointly support high-risk, high-reward research at the intersection of thesocial and technical dimensions of AI</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.319307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Priority will be given to collaborative projects thatintegrate computer/computational science with the social, behavioral, and economicsciences</td>\n",
       "      <td>Adherence bias</td>\n",
       "      <td>0.334865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Proposals may expand understanding of the influences of AI on people andsociety or contribute technical innovations that overcome the emerging social challenges</td>\n",
       "      <td>Racial bias</td>\n",
       "      <td>0.447941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1This document has been archived</td>\n",
       "      <td>Information bias</td>\n",
       "      <td>0.309076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Topics include, but are not limited to:Safety, robustness, and accountability of AI systems;Bias and fairness of AI systems;Intelligibility, explanation, and transparency of AI inferences;Privacy challenges with AI development and use;Sociotechnical challenges involving ethical considerations;Economic impacts of AI on society; andSocial consequences of AI system deployments</td>\n",
       "      <td>Racial bias</td>\n",
       "      <td>0.448541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EAGER proposals that fail to address concepts described in this DCL will be returned withoutreview</td>\n",
       "      <td>Hot stuff bias</td>\n",
       "      <td>0.306175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>An individual may appear as principal investigator (PI), co-PI, Senior Personnel, orConsultant on no more than one  EAGER proposal submitted in response to this DCL</td>\n",
       "      <td>Previous opinion bias</td>\n",
       "      <td>0.365337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Community response to this DCL will help identify ground-breaking directions</td>\n",
       "      <td>Detection bias</td>\n",
       "      <td>0.284295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>In parallel, theComputing Community Consortium (CCC)  is leading an AI \"roadmapping\" effort that willcomplement the EAGERs funded pursuant to this DCL</td>\n",
       "      <td>Immortal time bias</td>\n",
       "      <td>0.202791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>That roadmapping activity aims tobuild community consensus around grand challenges in AI and an associated long-term andinterdisciplinary research agenda</td>\n",
       "      <td>Immortal time bias</td>\n",
       "      <td>0.380476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EAGER proposals pursuant to this DCL are welcome through February 28, 2019 , but earlier submissions are encouraged</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.212903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Submissions should follow the guidance in NSF Proposal &amp; Award Policies &amp; Procedures Guide  (PAPPG) Chapter II</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.353227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NSF and the Partnership on AI anticipate jointly supporting up to 15 EAGER awards, each up to $300,000  for up to two years, in accordance with the PAPPG</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.327382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>EAGER proposals pursuant to this DCL  must include the prefix \"AI-DCL:\" in front of the title</td>\n",
       "      <td>Unacceptable disease bias</td>\n",
       "      <td>0.219697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Once NSF program officers have approved the prospectus, the PIwill be invited to submit a full EAGER proposal to a specific program</td>\n",
       "      <td>Verification bias</td>\n",
       "      <td>0.299140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The prospectus shouldbe responsive to the DCL and make a compelling case that the project is suitable for anEAGER</td>\n",
       "      <td>Industry Sponsorship bias</td>\n",
       "      <td>0.239901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The prospectus must be received no later than January 9, 2019</td>\n",
       "      <td>Unacceptable disease bias</td>\n",
       "      <td>0.187632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sincerely,Jim KuroseAssistant Director, CISE2Arthur LupiaAssistant Director, SBE 3</td>\n",
       "      <td>Spin bias</td>\n",
       "      <td>0.108736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>To reduce the use of information that merely supports an individual’s ideas, AI models must be trained on datasets encompassing diverse and potentially contradictory viewpoints to mitigate confirmation bias.</td>\n",
       "      <td>Confirmation bias</td>\n",
       "      <td>0.504719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AI systems must prioritize the relevance and quality of the data used over mere volume, to prevent bias arising from differences in the collection of information</td>\n",
       "      <td>Information bias</td>\n",
       "      <td>0.498263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>To address distortions caused by prejudice against racial or ethnic groups, developers must audit training datasets to ensure no group is underrepresented or mischaracterized, thereby reducing racial bias in AI outcomes.</td>\n",
       "      <td>Selection bias</td>\n",
       "      <td>0.498673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>To mitigate selection bias, which occurs when sampled groups differ systematically from the target population, training datasets must be representative of the diversity within the actual end-user base.</td>\n",
       "      <td>Selection bias</td>\n",
       "      <td>0.611245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AI policy requires full transparency in documenting dataset sources and limitations, to prevent bias caused by selective reporting or omission of information.</td>\n",
       "      <td>Reporting biases</td>\n",
       "      <td>0.517353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Text Chunk  \\\n",
       "0   NATIONAL SCIENCE FOUNDATION2415 EISENHOWER AVENUEALEXANDRIA, VIRGINIA 22314NSF 19-018Dear Colleague Letter: EArly-concept Grants for ExploratoryResearch on Artificial Intelligence (AI) and Society - SupportedJointly with the Partnership on AINovember 1 5, 2018Dear Colleagues:The National Science Foundation's (NSF) Directorates for Computer and Information Scienceand Engineering (CISE) and Social, Behavioral and Economic Sciences (SBE) together withthe Partnership on AI (PAI) wish to notify the community of their interest in supporting EArly-concept Grants for Exploratory Research (EAGERs) to understand the social challengesarising from AI technology and enable scientific contributions to overcome them   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The last 20 years have seen rapid advances in machine learning, pattern recognition,planning, effective decision making, natural language processing, and machine vision   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Theseadvances have been fueled by increased data, faster computation, and improved algorithms   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  They are yielding increasingly diverse and large-scale applications deployed in settingssubject to unanticipated challenges with complex social effects   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       NSF has long supported fundamental research enabling AI technology   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  With increases in thescale and diversity of deployments comes the need to better understand AI in the open world,including unforeseen circumstances and social impacts, and to craft approaches to AI thatconsider these from the start   \n",
       "6                                                                                                                                                                                                                                                                                                                                               Vital directions include developing principles for safe, robust,and trustworthy AI (including shared responsibilities between humans and AI systems);addressing issues of bias, fairness, and transparency of algorithmic intelligence; developingdeeper understanding of human-AI interaction and user education; and developing insightsabout the influences of AI on people and society   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         NSF and PAI will jointly support high-risk, high-reward research at the intersection of thesocial and technical dimensions of AI   \n",
       "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Priority will be given to collaborative projects thatintegrate computer/computational science with the social, behavioral, and economicsciences   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Proposals may expand understanding of the influences of AI on people andsociety or contribute technical innovations that overcome the emerging social challenges   \n",
       "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        1This document has been archived   \n",
       "11                                                                                                                                                                                                                                                                                                                                                Topics include, but are not limited to:Safety, robustness, and accountability of AI systems;Bias and fairness of AI systems;Intelligibility, explanation, and transparency of AI inferences;Privacy challenges with AI development and use;Sociotechnical challenges involving ethical considerations;Economic impacts of AI on society; andSocial consequences of AI system deployments   \n",
       "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      EAGER proposals that fail to address concepts described in this DCL will be returned withoutreview   \n",
       "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    An individual may appear as principal investigator (PI), co-PI, Senior Personnel, orConsultant on no more than one  EAGER proposal submitted in response to this DCL   \n",
       "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Community response to this DCL will help identify ground-breaking directions   \n",
       "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In parallel, theComputing Community Consortium (CCC)  is leading an AI \"roadmapping\" effort that willcomplement the EAGERs funded pursuant to this DCL   \n",
       "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               That roadmapping activity aims tobuild community consensus around grand challenges in AI and an associated long-term andinterdisciplinary research agenda   \n",
       "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     EAGER proposals pursuant to this DCL are welcome through February 28, 2019 , but earlier submissions are encouraged   \n",
       "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Submissions should follow the guidance in NSF Proposal & Award Policies & Procedures Guide  (PAPPG) Chapter II   \n",
       "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               NSF and the Partnership on AI anticipate jointly supporting up to 15 EAGER awards, each up to $300,000  for up to two years, in accordance with the PAPPG   \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           EAGER proposals pursuant to this DCL  must include the prefix \"AI-DCL:\" in front of the title   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Once NSF program officers have approved the prospectus, the PIwill be invited to submit a full EAGER proposal to a specific program   \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       The prospectus shouldbe responsive to the DCL and make a compelling case that the project is suitable for anEAGER   \n",
       "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The prospectus must be received no later than January 9, 2019   \n",
       "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Sincerely,Jim KuroseAssistant Director, CISE2Arthur LupiaAssistant Director, SBE 3   \n",
       "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To reduce the use of information that merely supports an individual’s ideas, AI models must be trained on datasets encompassing diverse and potentially contradictory viewpoints to mitigate confirmation bias.   \n",
       "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       AI systems must prioritize the relevance and quality of the data used over mere volume, to prevent bias arising from differences in the collection of information   \n",
       "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            To address distortions caused by prejudice against racial or ethnic groups, developers must audit training datasets to ensure no group is underrepresented or mischaracterized, thereby reducing racial bias in AI outcomes.   \n",
       "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               To mitigate selection bias, which occurs when sampled groups differ systematically from the target population, training datasets must be representative of the diversity within the actual end-user base.   \n",
       "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI policy requires full transparency in documenting dataset sources and limitations, to prevent bias caused by selective reporting or omission of information.   \n",
       "\n",
       "           Best Matching Bias  Similarity Score  \n",
       "0   Industry Sponsorship bias          0.280374  \n",
       "1            Information bias          0.304363  \n",
       "2            Information bias          0.300446  \n",
       "3                 Racial bias          0.366127  \n",
       "4   Industry Sponsorship bias          0.339133  \n",
       "5                 Racial bias          0.363890  \n",
       "6                 Racial bias          0.450619  \n",
       "7   Industry Sponsorship bias          0.319307  \n",
       "8              Adherence bias          0.334865  \n",
       "9                 Racial bias          0.447941  \n",
       "10           Information bias          0.309076  \n",
       "11                Racial bias          0.448541  \n",
       "12             Hot stuff bias          0.306175  \n",
       "13      Previous opinion bias          0.365337  \n",
       "14             Detection bias          0.284295  \n",
       "15         Immortal time bias          0.202791  \n",
       "16         Immortal time bias          0.380476  \n",
       "17  Industry Sponsorship bias          0.212903  \n",
       "18  Industry Sponsorship bias          0.353227  \n",
       "19  Industry Sponsorship bias          0.327382  \n",
       "20  Unacceptable disease bias          0.219697  \n",
       "21          Verification bias          0.299140  \n",
       "22  Industry Sponsorship bias          0.239901  \n",
       "23  Unacceptable disease bias          0.187632  \n",
       "24                  Spin bias          0.108736  \n",
       "25          Confirmation bias          0.504719  \n",
       "26           Information bias          0.498263  \n",
       "27             Selection bias          0.498673  \n",
       "28             Selection bias          0.611245  \n",
       "29           Reporting biases          0.517353  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
